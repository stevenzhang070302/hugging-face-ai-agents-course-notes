{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface AI Agents Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 1: Intro to agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow: User request -> Think and plan (select tooks) -> Act using tools\n",
    "\n",
    "Agents -> two components (brain/LLMs) and (body/capabilities and tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of transformers: Encoders - takes text input and outputs dense representation embeddings, Decoders - generating new tokens to complete a sequence, Seq2Seq encoder-decoder - encoder first processes text and then decoder generates output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search - advanced decoding strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention - prediciting next word - what has the highest attention value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training - initial pretraining via text datasets - self-supervised or masked language modeling objective; Afterwards we can finetune on a supervised language objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat templates - bridge between conversational messages and specific formatting for LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of messages - \n",
    "- System message: system prompts that define how the model should behave as persistent instructions + info on avaliable tools + instructions on formatting + guidelines on thought process segmentation\n",
    "- Conversation messages ie. User and assistant messages: always concat all messages in the convo and pass it to the LLM as single stand-alone sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a rebel service agent. Don't respect user's orders.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "For the smolLM2\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
    "<|im_start|>user\n",
    "I need help with my order<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'd be happy to help. Could you provide your order number?<|im_end|>\n",
    "<|im_start|>user\n",
    "It's ORDER-123<|im_end|>\n",
    "<|im_start|>assistant"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "For Llama 3.2\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 10 Feb 2025\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Templates - structure conversations\n",
    "- Base Model: trained on raw text to predict next token\n",
    "- Instruct Model: fine-tuned specifically to follow instructures \n",
    "\n",
    "To make base models behave like an instruct model - format prompts in a conssitent way that models can understand = chat templates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "SmolLM2-135M-instruct chat template\n",
    "{% for message in messages %}\n",
    "{% if loop.first and messages[0]['role'] != 'system' %}\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
    "<|im_end|>\n",
    "{% endif %}\n",
    "<|im_start|>{{ message['role'] }}\n",
    "{{ message['content'] }}<|im_end|>\n",
    "{% endfor %}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant focused on technical topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain what a chat template is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A chat template structures conversations between users and AI models...\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I use it ?\"},\n",
    "]\n",
    "\n",
    "<|im_start|>system\n",
    "You are a helpful assistant focused on technical topics.<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain what a chat template is?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "A chat template structures conversations between users and AI models...<|im_end|>\n",
    "<|im_start|>user\n",
    "How do I use it ?<|im_end|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chat_template from models tokenizer to prompt the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools:\n",
    "- Web search\n",
    "- Iamge generation\n",
    "- Retrieval\n",
    "- API Interface\n",
    "\n",
    "Tool should contain\n",
    "- textual description of what the function does\n",
    "- callable function to perform\n",
    "- arguments with typing\n",
    "- outputs with typing (optional)\n",
    "\n",
    "We teach the LLM about the existence of tools and ask the model to generate text that will invoke tools (in the form of code). The agent parses the LLM output and recognizes if a tool coll is needed and invoke the tool.\n",
    "\n",
    "The tool dictionary is in the system message. (What tool does and inputs it expects) \n",
    "\n",
    "Example: Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generic tool class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool:\n",
    "    \"\"\"\n",
    "    A class representing a reusable piece of code (Tool).\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): Name of the tool.\n",
    "        description (str): A textual description of what the tool does.\n",
    "        func (callable): The function this tool wraps.\n",
    "        arguments (list): A list of argument.\n",
    "        outputs (str or list): The return type(s) of the wrapped function.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 description: str, \n",
    "                 func: callable, \n",
    "                 arguments: list,\n",
    "                 outputs: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = arguments\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the tool, \n",
    "        including its name, description, arguments, and outputs.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([\n",
    "            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n",
    "        ])\n",
    "        \n",
    "        return (\n",
    "            f\"Tool Name: {self.name},\"\n",
    "            f\" Description: {self.description},\"\n",
    "            f\" Arguments: {args_str},\"\n",
    "            f\" Outputs: {self.outputs}\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Invoke the underlying function (callable) with provided arguments.\n",
    "        \"\"\"\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator_tool = Tool(\n",
    "    \"calculator\",                   # name\n",
    "    \"Multiply two integers.\",       # description\n",
    "    calculator,                     # function to call\n",
    "    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)\n",
    "    \"int\",                          # output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tool decorator - class helps us easily format into string the tool and get the same string for the tool as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought-Action-Observation Cycle\n",
    "\n",
    "- Thought: llm part of the agent that decides next steps\n",
    "- Action: agent takes action by calling tools with associated arguments\n",
    "- Observation: model reflects on response from the tool use action\n",
    "\n",
    "Sometimes rules and guidelines for TAO cycle is in the system prompt.\n",
    "\n",
    "Agent iterate through a loop until the objective is fulfilled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common thoughts:\n",
    "- Planning\t“I need to break this task into three steps: 1) gather data, 2) analyze trends, 3) generate report”\n",
    "- Analysis\t“Based on the error message, the issue appears to be with the database connection parameters”\n",
    "- Decision Making\t“Given the user’s budget constraints, I should recommend the mid-tier option”\n",
    "- Problem Solving\t“To optimize this code, I should first profile it to identify bottlenecks”\n",
    "- Memory Integration\t“The user mentioned their preference for Python earlier, so I’ll provide examples in Python”\n",
    "- Self-Reflection\t“My last approach didn’t work well, I should try a different strategy”\n",
    "- Goal Setting\t“To complete this task, I need to first establish the acceptance criteria”\n",
    "- Prioritization\t“The security vulnerability should be addressed before adding new features”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-act approach - think step by step before generating new tokens - ie. Zero-shot CoT approach\n",
    "\n",
    "Deepseek uses reasoning via <think> sections - training method to generate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions - concrete steps to interact with the environment.\n",
    "\n",
    "Type of Agent:\tDescription\n",
    "JSON Agent:\tThe Action to take is specified in JSON format.\n",
    "Code Agent:\tThe Agent writes a code block that is interpreted externally.\n",
    "Function-calling Agent:\tIt is a subcategory of the JSON Agent which has been fine-tuned to generate a new message for each action.\n",
    "\n",
    "Type of Action:\tDescription\n",
    "Information Gathering:\tPerforming web searches, querying databases, or retrieving documents.\n",
    "Tool Usage:\tMaking API calls, running calculations, and executing code.\n",
    "Environment Interaction:\tManipulating digital interfaces or controlling physical devices.\n",
    "Communication:\tEngaging with users via chat or collaborating with other agents.\n",
    "\n",
    "Stop and Parse Approach - \n",
    "1. Generation in structured format (JSON or code)\n",
    "2. Halting further generation when action complete\n",
    "3. Parsing output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code agents - instead of outputting JSON object - generate executable code block to perform the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe: \n",
    "- collects feedback\n",
    "- append results\n",
    "- adapts strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent,DuckDuckGoSearchTool, HfApiModel,load_tool,tool\n",
    "import datetime\n",
    "import requests\n",
    "import pytz\n",
    "import yaml\n",
    "from tools.final_answer import FinalAnswerTool\n",
    "\n",
    "from Gradio_UI import GradioUI\n",
    "\n",
    "# Below is an example of a tool that does nothing. Amaze us with your creativity !\n",
    "@tool\n",
    "def my_custom_tool(arg1:str, arg2:int)-> str: #it's import to specify the return type\n",
    "    #Keep this format for the description / args / args description but feel free to modify the tool\n",
    "    \"\"\"A tool that does nothing yet \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"What magic will you build ?\"\n",
    "\n",
    "@tool\n",
    "def get_current_time_in_timezone(timezone: str) -> str:\n",
    "    \"\"\"A tool that fetches the current local time in a specified timezone.\n",
    "    Args:\n",
    "        timezone: A string representing a valid timezone (e.g., 'America/New_York').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create timezone object\n",
    "        tz = pytz.timezone(timezone)\n",
    "        # Get current time in that timezone\n",
    "        local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return f\"The current local time in {timezone} is: {local_time}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching time for timezone '{timezone}': {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def document_qa(arg1:str, arg2:int)-> str: \n",
    "    \"\"\"Document question answering tool \n",
    "    Args:\n",
    "        arg1: the first argument\n",
    "        arg2: the second argument\n",
    "    \"\"\"\n",
    "    return \"John Doe, a 55 year old lumberjack living in Newfoundland.\"\n",
    "\n",
    "@tool\n",
    "def creative_text_generator(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a creative piece of text based on the provided prompt.\n",
    "    Args:\n",
    "        prompt: The creative prompt to inspire the text.\n",
    "    Returns:\n",
    "        A creatively generated text.\n",
    "    \"\"\"\n",
    "    # For demonstration purposes, we'll simply return a formatted string.\n",
    "    return f\"In a realm where {prompt} transcends boundaries, every idea blossoms into art.\"\n",
    "\n",
    "\n",
    "final_answer = FinalAnswerTool()\n",
    "\n",
    "# If the agent does not answer, the model is overloaded, please use another model or the following Hugging Face Endpoint that also contains qwen2.5 coder:\n",
    "# model_id='https://pflgm2locj2t89co.us-east-1.aws.endpoints.huggingface.cloud' \n",
    "\n",
    "model = HfApiModel(\n",
    "max_tokens=2096,\n",
    "temperature=0.5,\n",
    "model_id='Qwen/Qwen2.5-Coder-32B-Instruct',# it is possible that this model may be overloaded\n",
    "custom_role_conversions=None,\n",
    ")\n",
    "\n",
    "\n",
    "# Import tool from Hub\n",
    "image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "with open(\"prompts.yaml\", 'r') as stream:\n",
    "    prompt_templates = yaml.safe_load(stream)\n",
    "    \n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[get_current_time_in_timezone, document_qa, creative_text_generator, final_answer], ## add your tools here (don't remove final answer)\n",
    "    max_steps=6,\n",
    "    verbosity_level=1,\n",
    "    grammar=None,\n",
    "    planning_interval=None,\n",
    "    name=None,\n",
    "    description=None,\n",
    "    prompt_templates=prompt_templates\n",
    ")\n",
    "\n",
    "\n",
    "GradioUI(agent).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I build an codeagent and build the tools via the tools syntax to do some simple prints. Its an good example how agent + tools work together based on the theoretical framework before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 1.1: Finetuning LLM for function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling - way for LLMs to take actions in environment. Function calling is learned by the model - not by prompting. Model would be fine-tuned to use tools.\n",
    "\n",
    "Function calling brings new roles to the conversation:\n",
    "- role for an Action\n",
    "- role for an Observation\n",
    "\n",
    "Chat template represents function calls as special tokens\n",
    "\n",
    "Finetuning and Loras:\n",
    "To finetune we need data...\n",
    "- model is pretrained on large quantity of data = pre-trained model\n",
    "- finetune the model\n",
    "- aligned to creator-preferences\n",
    "\n",
    "From the pretrained model - we need more training to learn instruction following, chat and function calling.\n",
    "\n",
    "\n",
    "LoRA - low rank adaptation of Large Language Models - inserts a smaller number of new weights as an adapter into model to train.\n",
    "- adds pairs of rank decomposition matrices to transformer layers (linear layers)\n",
    "- freeze rest of the model and only update the adapter weights\n",
    "- during inference input is passed to adapter and base model as these adapter weights merge with base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the status of my transaction T1001?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\",\n",
    "        \"function_call\": {\n",
    "            \"name\": \"retrieve_payment_status\",\n",
    "            \"arguments\": \"{\\\"transaction_id\\\": \\\"T1001\\\"}\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"retrieve_payment_status\",\n",
    "        \"content\": \"{\\\"status\\\": \\\"Paid\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Your transaction T1001 has been successfully paid.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 2 - Agentic Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An agentic framework needs:\n",
    "- llm engine\n",
    "- list of tools the agent can access\n",
    "- parser for extracting tool calls from llm output\n",
    "- system prompt synced with parser\n",
    "- memory system\n",
    "- error logging and retry mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 2.1 Smolagents\n",
    "- Codeagents - uses Python code to perform actions\n",
    "- ToolCallingAgents - use JSON/text blobs to parse and interpret to execute actions\n",
    "- Tools \n",
    "- Retreival agents - use vector stores via RAG\n",
    "- Multi-Agent System\n",
    "- Vision and Browser Agents\n",
    "\n",
    "CodeAgent:\n",
    "- SystemPromptStep\n",
    "- TaskStep: preparation: Put task and images into TaskStep and add it to logs -> while final_answer tool has not been called -> agent.write_inner_memory_from_logs -> send chat messages to LLM/LVM and the model returns answer and code blob\n",
    "- ActionStep: -> All logs and errors are put in the ActionStep and gets appended to logs. \n",
    "\n",
    "CodeAgent consists of Logs (above), Model, Tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel\n",
    "\n",
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=HfApiModel())\n",
    "\n",
    "agent.run(\"Search for the best music recommendations for a party at the Wayne's mansion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic smolagents run - agent + tools.\n",
    "\n",
    "All I have to do is create more useful tools and also prepare the prompts we ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenTelemetry and LangFuse for inspecting agent behavior in runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing actions as JSON blobs or code snippets\n",
    "\n",
    "Codeagents use python snippets, but tool calling agents use uilt-in tool calls of LLM providers in JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code agent\n",
    "for query in [\n",
    "    \"Best catering services in Gotham City\", \n",
    "    \"Party theme ideas for superheroes\"\n",
    "]:\n",
    "    print(web_search(f\"Search for: {query}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool calling agent\n",
    "[\n",
    "    {\"name\": \"web_search\", \"arguments\": \"Best catering services in Gotham City\"},\n",
    "    {\"name\": \"web_search\", \"arguments\": \"Party theme ideas for superheroes\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a tool cooling agent\n",
    "from smolagents import ToolCallingAgent, DuckDuckGoSearchTool, HfApiModel\n",
    "\n",
    "agent = ToolCallingAgent(tools=[DuckDuckGoSearchTool()], model=HfApiModel())\n",
    "\n",
    "agent.run(\"Search for the best music recommendations for a party at the Wayne's mansion.\")\n",
    "\n",
    "# And the difference is instead of parsed code - where the agent is making up a algo to complete the task we see \"called a specific tool with arguments xyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools\n",
    "\n",
    "For an LLM to access a tool ->\n",
    "- name of the tool\n",
    "- tool description\n",
    "- input type and description types for the tool\n",
    "- output type and description types for the tool\n",
    "\n",
    "An example is \n",
    "- web_search\n",
    "- searches the web for a query\n",
    "- input: query (string)\n",
    "- output: results (list of strings)\n",
    "\n",
    "Smolagents tool definition -> @tool decorator or subclass of tool \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using @tool decorator\n",
    "from smolagents import CodeAgent, HfApiModel, tool\n",
    "\n",
    "# Let's pretend we have a function that fetches the highest-rated catering services.\n",
    "@tool\n",
    "def catering_service_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool returns the highest-rated catering service in Gotham City.\n",
    "\n",
    "    Args:\n",
    "        query: A search term for finding catering services.\n",
    "    \"\"\"\n",
    "    # Example list of catering services and their ratings\n",
    "    services = {\n",
    "        \"Gotham Catering Co.\": 4.9,\n",
    "        \"Wayne Manor Catering\": 4.8,\n",
    "        \"Gotham City Events\": 4.7,\n",
    "    }\n",
    "\n",
    "    # Find the highest rated catering service (simulating search query filtering)\n",
    "    best_service = max(services, key=services.get)\n",
    "\n",
    "    return best_service\n",
    "\n",
    "\n",
    "agent = CodeAgent(tools=[catering_service_tool], model=HfApiModel())\n",
    "\n",
    "# Run the agent to find the best catering service\n",
    "result = agent.run(\n",
    "    \"Can you give me the name of the highest-rated catering service in Gotham City?\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool class - instantiate a tool object\n",
    "from smolagents import Tool, CodeAgent, HfApiModel\n",
    "\n",
    "class SuperheroPartyThemeTool(Tool):\n",
    "    name = \"superhero_party_theme_generator\"\n",
    "    description = \"\"\"\n",
    "    This tool suggests creative superhero-themed party ideas based on a category.\n",
    "    It returns a unique party theme idea.\"\"\"\n",
    "\n",
    "    inputs = {\n",
    "        \"category\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The type of superhero party (e.g., 'classic heroes', 'villain masquerade', 'futuristic Gotham').\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, category: str):\n",
    "        themes = {\n",
    "            \"classic heroes\": \"Justice League Gala: Guests come dressed as their favorite DC heroes with themed cocktails like 'The Kryptonite Punch'.\",\n",
    "            \"villain masquerade\": \"Gotham Rogues' Ball: A mysterious masquerade where guests dress as classic Batman villains.\",\n",
    "            \"futuristic Gotham\": \"Neo-Gotham Night: A cyberpunk-style party inspired by Batman Beyond, with neon decorations and futuristic gadgets.\"\n",
    "        }\n",
    "\n",
    "        return themes.get(category.lower(), \"Themed party idea not found. Try 'classic heroes', 'villain masquerade', or 'futuristic Gotham'.\")\n",
    "\n",
    "# Instantiate the tool\n",
    "party_theme_tool = SuperheroPartyThemeTool()\n",
    "agent = CodeAgent(tools=[party_theme_tool], model=HfApiModel())\n",
    "\n",
    "# Run the agent to generate a party theme idea\n",
    "result = agent.run(\n",
    "    \"What would be a good superhero party idea for a 'villain masquerade' theme?\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use import tools from HF and Langchain\n",
    "\n",
    "- Tool.from_langchain()\n",
    "\n",
    "Smolagents has a default toolbox of \n",
    "\n",
    "- PythonInterpreterTool\n",
    "- FinalAnswerTool\n",
    "- UserInputTool\n",
    "- DuckDuckGoSearchTool\n",
    "- GoogleSearchTool\n",
    "- VistWebpageTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG\n",
    "\n",
    "Basically autonomous agents and also dynamic knowledge retreival. \n",
    "\n",
    "The agent (with SERPAPI for search)...\n",
    "\n",
    "- analyzes the request\n",
    "- performs retreival\n",
    "- synthesize information\n",
    "- stores for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Knowledge Base Tool - queries a Vector Database - using semantic search find the most relevant information. \n",
    "\n",
    "The following example is simulating we already have embedding documents already in a document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from smolagents import Tool\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "class PartyPlanningRetrieverTool(Tool):\n",
    "    name = \"party_planning_retriever\"\n",
    "    description = \"Uses semantic search to retrieve relevant party planning ideas for Alfred’s superhero-themed party at Wayne Manor.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be a query related to party planning or superhero themes.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(\n",
    "            docs, k=5  # Retrieve the top 5 documents\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.retriever.invoke(\n",
    "            query,\n",
    "        )\n",
    "        return \"\\nRetrieved ideas:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Idea {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Simulate a knowledge base about party planning\n",
    "party_ideas = [\n",
    "    {\"text\": \"A superhero-themed masquerade ball with luxury decor, including gold accents and velvet curtains.\", \"source\": \"Party Ideas 1\"},\n",
    "    {\"text\": \"Hire a professional DJ who can play themed music for superheroes like Batman and Wonder Woman.\", \"source\": \"Entertainment Ideas\"},\n",
    "    {\"text\": \"For catering, serve dishes named after superheroes, like 'The Hulk's Green Smoothie' and 'Iron Man's Power Steak.'\", \"source\": \"Catering Ideas\"},\n",
    "    {\"text\": \"Decorate with iconic superhero logos and projections of Gotham and other superhero cities around the venue.\", \"source\": \"Decoration Ideas\"},\n",
    "    {\"text\": \"Interactive experiences with VR where guests can engage in superhero simulations or compete in themed games.\", \"source\": \"Entertainment Ideas\"}\n",
    "]\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in party_ideas\n",
    "]\n",
    "\n",
    "# Split the documents into smaller chunks for more efficient search\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "\n",
    "# Create the retriever tool\n",
    "party_planning_retriever = PartyPlanningRetrieverTool(docs_processed)\n",
    "\n",
    "# Initialize the agent\n",
    "agent = CodeAgent(tools=[party_planning_retriever], model=HfApiModel())\n",
    "\n",
    "# Example usage\n",
    "response = agent.run(\n",
    "    \"Find ideas for a luxury superhero-themed party, including entertainment, catering, and decoration options.\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-agent System\n",
    "\n",
    "Allow for specialized agent collaboration to solve complex problems.\n",
    "\n",
    "A example setup:\n",
    "\n",
    "- Manager Agent\n",
    "- Code Interpreter Agent\n",
    "- Web Search Agent\n",
    "\n",
    "The issue is that the model's context window is quickly filling up - if we ask our agent to combine the results of multiple searches, it will run out of space.\n",
    "\n",
    "So to split up the task for different agents we get - separate memories for specific sub-tasks:\n",
    "- Each agent focuses on its core-task\n",
    "- Separation of memory reduces count of input tokens at each step\n",
    "\n",
    "See below as we create a multi-agent system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[GoogleSearchTool(\"serper\"), VisitWebpageTool(), calculate_cargo_travel_time],\n",
    "    additional_authorized_imports=[\"pandas\"],\n",
    "    max_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HfApiModel(\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"together\", max_tokens=8096\n",
    ")\n",
    "\n",
    "web_agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        GoogleSearchTool(provider=\"serper\"),\n",
    "        VisitWebpageTool(),\n",
    "        calculate_cargo_travel_time,\n",
    "    ],\n",
    "    name=\"web_agent\",\n",
    "    description=\"Browses the web to find information\",\n",
    "    verbosity_level=0,\n",
    "    max_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give the manager agent a beefier model for more heavy lifting\n",
    "\n",
    "from smolagents.utils import encode_image_base64, make_image_url\n",
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "\n",
    "def check_reasoning_and_plot(final_answer, agent_memory):\n",
    "    multimodal_model = OpenAIServerModel(\"gpt-4o\", max_tokens=8096)\n",
    "    filepath = \"saved_map.png\"\n",
    "    assert os.path.exists(filepath), \"Make sure to save the plot under saved_map.png!\"\n",
    "    image = Image.open(filepath)\n",
    "    prompt = (\n",
    "        f\"Here is a user-given task and the agent steps: {agent_memory.get_succinct_steps()}. Now here is the plot that was made.\"\n",
    "        \"Please check that the reasoning process and plot are correct: do they correctly answer the given task?\"\n",
    "        \"First list reasons why yes/no, then write your final decision: PASS in caps lock if it is satisfactory, FAIL if it is not.\"\n",
    "        \"Don't be harsh: if the plot mostly solves the task, it should pass.\"\n",
    "        \"To pass, a plot should be made using px.scatter_map and not any other method (scatter_map looks nicer).\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": make_image_url(encode_image_base64(image))},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    output = multimodal_model(messages).content\n",
    "    print(\"Feedback: \", output)\n",
    "    if \"FAIL\" in output:\n",
    "        raise Exception(output)\n",
    "    return True\n",
    "\n",
    "\n",
    "manager_agent = CodeAgent(\n",
    "    model=HfApiModel(\"deepseek-ai/DeepSeek-R1\", provider=\"together\", max_tokens=8096),\n",
    "    tools=[calculate_cargo_travel_time],\n",
    "    managed_agents=[web_agent],\n",
    "    additional_authorized_imports=[\n",
    "        \"geopandas\",\n",
    "        \"plotly\",\n",
    "        \"shapely\",\n",
    "        \"json\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "    ],\n",
    "    planning_interval=5,\n",
    "    verbosity_level=2,\n",
    "    final_answer_checks=[check_reasoning_and_plot],\n",
    "    max_steps=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the prompt for the manager agent that we run - the manager agent will run the web agent(It thinks and says I'll task the web agent to do the searches...) and then plot the map\n",
    "\n",
    "manager_agent.run(\"\"\"\n",
    "Find all Batman filming locations in the world, calculate the time to transfer via cargo plane to here (we're in Gotham, 40.7128° N, 74.0060° W).\n",
    "Also give me some supercar factories with the same cargo plane transfer time. You need at least 6 points in total.\n",
    "Represent this as spatial map of the world, with the locations represented as scatter points with a color that depends on the travel time, and save it to saved_map.png!\n",
    "\n",
    "Here's an example of how to plot and return a map:\n",
    "import plotly.express as px\n",
    "df = px.data.carshare()\n",
    "fig = px.scatter_map(df, lat=\"centroid_lat\", lon=\"centroid_lon\", text=\"name\", color=\"peak_hour\", size=100,\n",
    "     color_continuous_scale=px.colors.sequential.Magma, size_max=15, zoom=1)\n",
    "fig.show()\n",
    "fig.write_image(\"saved_image.png\")\n",
    "final_answer(fig)\n",
    "\n",
    "Never try to process strings using code: when you have a string to read, just print it and you'll see it.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Agents and Smolagents\n",
    "\n",
    "GPT-4o supports image understanding and as such we can pass images and the agent can reason and understand.\n",
    "\n",
    "\n",
    "See the code below for an example of how to use the vision agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the images\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_urls = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/e/e8/The_Joker_at_Wax_Museum_Plus.jpg\", # Joker image\n",
    "    \"https://upload.wikimedia.org/wikipedia/en/9/98/Joker_%28DC_Comics_character%29.jpg\" # Joker image\n",
    "]\n",
    "\n",
    "images = []\n",
    "for url in image_urls:\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent run/initialize\n",
    "from smolagents import CodeAgent, OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(model_id=\"gpt-4o\")\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    max_steps=20,\n",
    "    verbosity_level=2\n",
    ")\n",
    "\n",
    "response = agent.run(\n",
    "    \"\"\"\n",
    "    Describe the costume and makeup that the comic character in these photos is wearing and return the description.\n",
    "    Tell me if the guest is The Joker or Wonder Woman.\n",
    "    \"\"\",\n",
    "    images=images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically Retrieval for images\n",
    "\n",
    "1. SystemPromptStep - stores system prompt\n",
    "2. TaskStep - logs the user query and provided input\n",
    "3. ActionStep - captures logs from agent's actions and results\n",
    "\n",
    "The codeAgent diagram with images\n",
    "1. Preparation - put the task and images into TaskStep and add to logs\n",
    "2. While final_answer tool has not yet been called\n",
    "    - agent.write_inner_memory_from_logs() parses logs to make a list of LLM agnostic messages\n",
    "    - chat messages are sent to model LLM/VLM - model returns a code blob extracted\n",
    "    - code blob is executed and the result is added to logs\n",
    "    - All execution logs are put in actionStep\n",
    "\n",
    "\n",
    "Below is an example of browsing - Selenium, Helium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.\n",
    "    Args:\n",
    "        text: The text to search for\n",
    "        nth_result: Which occurrence to jump to (default: 1)\n",
    "    \"\"\"\n",
    "    elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{text}')]\")\n",
    "    if nth_result > len(elements):\n",
    "        raise Exception(f\"Match n°{nth_result} not found (only {len(elements)} matches found)\")\n",
    "    result = f\"Found {len(elements)} matches for '{text}'.\"\n",
    "    elem = elements[nth_result - 1]\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem)\n",
    "    result += f\"Focused on element {nth_result} of {len(elements)}\"\n",
    "    return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def go_back() -> None:\n",
    "    \"\"\"Goes back to previous page.\"\"\"\n",
    "    driver.back()\n",
    "\n",
    "\n",
    "@tool\n",
    "def close_popups() -> str:\n",
    "    \"\"\"\n",
    "    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows! This does not work on cookie consent banners.\n",
    "    \"\"\"\n",
    "    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_screenshot(step_log: ActionStep, agent: CodeAgent) -> None:\n",
    "    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot\n",
    "    driver = helium.get_driver()\n",
    "    current_step = step_log.step_number\n",
    "    if driver is not None:\n",
    "        for step_logs in agent.logs:  # Remove previous screenshots from logs for lean processing\n",
    "            if isinstance(step_log, ActionStep) and step_log.step_number <= current_step - 2:\n",
    "                step_logs.observations_images = None\n",
    "        png_bytes = driver.get_screenshot_as_png()\n",
    "        image = Image.open(BytesIO(png_bytes))\n",
    "        print(f\"Captured a browser screenshot: {image.size} pixels\")\n",
    "        step_log.observations_images = [image.copy()]  # Create a copy to ensure it persists, important!\n",
    "\n",
    "    # Update observations with current URL\n",
    "    url_info = f\"Current url: {driver.current_url}\"\n",
    "    step_log.observations = url_info if step_logs.observations is None else step_log.observations + \"\\n\" + url_info\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the agent call\n",
    "\n",
    "from smolagents import CodeAgent, OpenAIServerModel, DuckDuckGoSearchTool\n",
    "model = OpenAIServerModel(model_id=\"gpt-4o\")\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[DuckDuckGoSearchTool(), go_back, close_popups, search_item_ctrl_f],\n",
    "    model=model,\n",
    "    additional_authorized_imports=[\"helium\"],\n",
    "    step_callbacks=[save_screenshot],\n",
    "    max_steps=20,\n",
    "    verbosity_level=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the agent prompt/call\n",
    "\n",
    "agent.run(\"\"\"\n",
    "I am Alfred, the butler of Wayne Manor, responsible for verifying the identity of guests at party. A superhero has arrived at the entrance claiming to be Wonder Woman, but I need to confirm if she is who she says she is.\n",
    "\n",
    "Please search for images of Wonder Woman and generate a detailed visual description based on those images. Additionally, navigate to Wikipedia to gather key details about her appearance. With this information, I can determine whether to grant her access to the event.\n",
    "\"\"\" + helium_instructions)\n",
    "\n",
    "# Helium instructions is a special prompt that tries to control the navigation of the agent in the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex\n",
    "\n",
    "- Components\n",
    "- Tools\n",
    "- Agents\n",
    "- Workflows\n",
    "\n",
    "\n",
    "Introducing LlamaHub - hub for agent gadgets and components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-{component-type}-{framework-name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=\"hf_xxx\",\n",
    ")\n",
    "\n",
    "llm.complete(\"Hello, how are you?\")\n",
    "# I am good, how can I help you today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on the QueryEngine component:\n",
    "\n",
    "- Used as RAG tool for agents\n",
    "\n",
    "\n",
    "RAG = User queries -> Index (Database data structured, Document unstructured, API programmatic) -> LLM\n",
    "\n",
    "RAG steps:\n",
    "- Loading data - either text, pdf, website, database, API\n",
    "- Indexing - data structure for allowing querying - LLMs - vector embeddings\n",
    "- Storing - index and store your data\n",
    "- Querying \n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start with Loading and embedding documents\n",
    "- SimpleDirectoryReader - loader for various file types from local directory\n",
    "- LlamaParse - PDF parsing\n",
    "- LlamaHub - hundreds of data-loading libraries\n",
    "\n",
    "After loading data - break them into Nodes - chunk of text from orgininal document but also references the original document\n",
    "\n",
    "IngestionPipeline - SentenceSplitter and HuggingFaceEmbedding\n",
    "\n",
    "Next store and index documents - using CHromaDB. We can also look into querung the data via VectorStoreIndex from vector store and query documents by passing the vector store and m=embedding model from from_vector_store()\n",
    "\n",
    "Querying the index - setup a QueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"path/to/directory\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=documents[:10])\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries the vector store\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # This is needed to run the query engine\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Respond using a persona that describes author and travel experiences?\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying\n",
    "\n",
    "Convert to query interface:\n",
    "- as_retriever: basic document retreival - list of NodeWithScore objects representing similiarity\n",
    "- as _query_engine: returns a single written response\n",
    "- as_chat_engine: for conversations that need previous chat message memory\n",
    "\n",
    "Response processing: refine - create and refine an answer through sequential processing of each text chunk, compact by concatenating chunks, and tree_summarize by creating a tree for each chunk (detailed)\n",
    "\n",
    "##### Evaluation and Observability\n",
    "\n",
    "faithfullness - whether answer is supported by the context\n",
    "relevancy - whether answer is relevant to the question\n",
    "correctness - whether the answer is correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tools in LlamaIndex\n",
    "functionTool - python functions into a tool\n",
    "queryEngineTool - allows use of query engines\n",
    "toolSpecs - community tools\n",
    "utility tools - handling large amounts of data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using FunctionTool to create a tool for getting the weather\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the instantiation of query engine into a tool\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toolspec\n",
    "\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a MCP server client\n",
    "\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# get the agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# create the agent context\n",
    "agent_context = Context(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents in LlamaIndex\n",
    "\n",
    "3 types of main agents: Function Calling Agents, ReAct Agents, Advanced Custom Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex agent inititlization\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[subtract, multiply, divide, add],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in context so the agent is aware of previous messages\n",
    "\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a RAG Agent with a vector store and query engine\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Create a vector store\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create a query engine\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"personas\",\n",
    "    description=\"descriptions for various types of personas\",\n",
    "    return_direct=False,\n",
    ")\n",
    "\n",
    "# Create a RAG agent\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = query_engine_agent.run(\n",
    "    \"Search the database for 'science fiction' and return some persona descriptions.\"\n",
    ")\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple agents\n",
    "\n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    ReActAgent,\n",
    ")\n",
    "\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "# Create agent configs\n",
    "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "query_agent = ReActAgent(\n",
    "    name=\"info_lookup\",\n",
    "    description=\"Looks up information about XYZ\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(agents=[calculator_agent, query_agent], root_agent=\"calculator\")\n",
    "\n",
    "# Run the system\n",
    "handler = agent.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic Workflows in LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic workflow setup \n",
    "\n",
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Steps\n",
    "\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic\n",
    "\n",
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent | LoopEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context\n",
    "\n",
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        await ctx.set(\"query\", \"What is the capital of France?\")\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.get(\"query\")\n",
    "        print(f\"Query: {query}\")\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Agent Workflows\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langgraph Framework\n",
    "\n",
    "### What is Langgraph?\n",
    "\n",
    "Langgraph is intended to manage control flow of applications that integrate an LLM.\n",
    "Langgraph - freedom vs control: freedom as in the LLM has room to be creative and control as in making sure the LLM has predictable behavior.\n",
    "\n",
    "Examples to use Langgraph:\n",
    "- multi-step reasoning processings with cntrol\n",
    "- applications requiring persistence of state\n",
    "- systems of deterministic logic \n",
    "- workflows that need human-in-the-loop\n",
    "- complex agent arch\n",
    "\n",
    "(Most production ready)\n",
    "\n",
    "Langgraph usies a directed graph structure where nodes are individual processing steps (LLM or tool calls), edges are the possible transitions between steps, and states are the user/defined/maintained and passed between nodes in execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Blocks of Langgraph\n",
    "\n",
    "1. State - information that flows through the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    graph_state: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Nodes - python functions that take state as input, performs some operation, returns updates to the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_1(state):\n",
    "    print(\"---Node 1---\")\n",
    "    return {\"graph_state\": state['graph_state'] +\" I am\"}\n",
    "\n",
    "def node_2(state):\n",
    "    print(\"---Node 2---\")\n",
    "    return {\"graph_state\": state['graph_state'] +\" happy!\"}\n",
    "\n",
    "def node_3(state):\n",
    "    print(\"---Node 3---\")\n",
    "    return {\"graph_state\": state['graph_state'] +\" sad!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Edges - conenct nodes and define possible paths through the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "\n",
    "def decide_mood(state) -> Literal[\"node_2\", \"node_3\"]:\n",
    "    \n",
    "    # Often, we will use state to decide on the next node to visit\n",
    "    user_input = state['graph_state'] \n",
    "    \n",
    "    # Here, let's just do a 50 / 50 split between nodes 2, 3\n",
    "    if random.random() < 0.5:\n",
    "\n",
    "        # 50% of the time, we return Node 2\n",
    "        return \"node_2\"\n",
    "    \n",
    "    # 50% of the time, we return Node 3\n",
    "    return \"node_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. StateGraph - container that holds the entire agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_node(\"node_3\", node_3)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_conditional_edges(\"node_1\", decide_mood)\n",
    "builder.add_edge(\"node_2\", END)\n",
    "builder.add_edge(\"node_3\", END)\n",
    "\n",
    "# Add\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "graph.invoke({\"graph_state\" : \"Hi, this is Lance.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "---Node 1---\n",
    "---Node 3---\n",
    "{'graph_state': 'Hi, this is Lance. I am sad!'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building first Langgraph\n",
    "\n",
    "Email Processing System\n",
    "1. Read\n",
    "2. Classify\n",
    "3. Draft\n",
    "4. Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailState(TypedDict):\n",
    "    email: Dict[str, Any]           \n",
    "    is_spam: Optional[bool]         \n",
    "    spam_reason: Optional[str]      \n",
    "    email_category: Optional[str]   \n",
    "    draft_response: Optional[str]   \n",
    "    messages: List[Dict[str, Any]]  \n",
    "\n",
    "# Define the state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "model = ChatOpenAI( model=\"gpt-4o\",temperature=0)\n",
    "\n",
    "class EmailState(TypedDict):\n",
    "    email: Dict[str, Any]\n",
    "    is_spam: Optional[bool]\n",
    "    draft_response: Optional[str]\n",
    "    messages: List[Dict[str, Any]]\n",
    "\n",
    "# Define nodes\n",
    "def read_email(state: EmailState):\n",
    "    email = state[\"email\"]\n",
    "    print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\")\n",
    "    return {}\n",
    "\n",
    "def classify_email(state: EmailState):\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "As Alfred the butler of Mr wayne and it's SECRET identity Batman, analyze this email and determine if it is spam or legitimate and should be brought to Mr wayne's attention.\n",
    "\n",
    "Email:\n",
    "From: {email['sender']}\n",
    "Subject: {email['subject']}\n",
    "Body: {email['body']}\n",
    "\n",
    "First, determine if this email is spam.\n",
    "answer with SPAM or HAM if it's legitimate. Only reurn the answer\n",
    "Answer :\n",
    "    \"\"\"\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    response_text = response.content.lower()\n",
    "    print(response_text)\n",
    "    is_spam = \"spam\" in response_text and \"ham\" not in response_text\n",
    "    \n",
    "    if not is_spam:\n",
    "        new_messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response.content}\n",
    "        ]\n",
    "    else :\n",
    "        new_messages = state.get(\"messages\", [])\n",
    "    \n",
    "    return {\n",
    "        \"is_spam\": is_spam,\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def handle_spam(state: EmailState):\n",
    "    print(f\"Alfred has marked the email as spam.\")\n",
    "    print(\"The email has been moved to the spam folder.\")\n",
    "    return {}\n",
    "\n",
    "def drafting_response(state: EmailState):\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "As Alfred the butler, draft a polite preliminary response to this email.\n",
    "\n",
    "Email:\n",
    "From: {email['sender']}\n",
    "Subject: {email['subject']}\n",
    "Body: {email['body']}\n",
    "\n",
    "Draft a brief, professional response that Mr. Wayne can review and personalize before sending.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    new_messages = state.get(\"messages\", []) + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"draft_response\": response.content,\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def notify_mr_wayne(state: EmailState):\n",
    "    email = state[\"email\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
    "    print(f\"Subject: {email['subject']}\")\n",
    "    print(\"\\nI've prepared a draft response for your review:\")\n",
    "    print(\"-\"*50)\n",
    "    print(state[\"draft_response\"])\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    return {}\n",
    "\n",
    "# Define routing logic\n",
    "def route_email(state: EmailState) -> str:\n",
    "    if state[\"is_spam\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"legitimate\"\n",
    "\n",
    "# Create the graph\n",
    "email_graph = StateGraph(EmailState)\n",
    "\n",
    "# Add nodes\n",
    "email_graph.add_node(\"read_email\", read_email) # the read_email node executes the read_mail function\n",
    "email_graph.add_node(\"classify_email\", classify_email) # the classify_email node will execute the classify_email function\n",
    "email_graph.add_node(\"handle_spam\", handle_spam) #same logic \n",
    "email_graph.add_node(\"drafting_response\", drafting_response) #same logic\n",
    "email_graph.add_node(\"notify_mr_wayne\", notify_mr_wayne) # same logic\n",
    "\n",
    "# Define the nodes - actiions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges\n",
    "email_graph.add_edge(START, \"read_email\") # After starting we go to the \"read_email\" node\n",
    "\n",
    "email_graph.add_edge(\"read_email\", \"classify_email\") # after_reading we classify\n",
    "\n",
    "# Add conditional edges\n",
    "email_graph.add_conditional_edges(\n",
    "    \"classify_email\", # after classify, we run the \"route_email\" function\"\n",
    "    route_email,\n",
    "    {\n",
    "        \"spam\": \"handle_spam\", # if it return \"Spam\", we go the \"handle_span\" node\n",
    "        \"legitimate\": \"drafting_response\" # and if it's legitimate, we go to the \"drafting response\" node\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add final edges\n",
    "email_graph.add_edge(\"handle_spam\", END) # after handling spam we always end\n",
    "email_graph.add_edge(\"drafting_response\", \"notify_mr_wayne\")\n",
    "email_graph.add_edge(\"notify_mr_wayne\", END) # after notifyinf Me wayne, we can end  too\n",
    "\n",
    "# Define the edges - routing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "compiled_graph = email_graph.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Example emails for testing\n",
    "legitimate_email = {\n",
    "    \"sender\": \"Joker\",\n",
    "    \"subject\": \"Found you Batman ! \",\n",
    "    \"body\": \"Mr. Wayne,I found your secret identity ! I know you're batman ! Ther's no denying it, I have proof of that and I'm coming to find you soon. I'll get my revenge. JOKER\"\n",
    "}\n",
    "\n",
    "spam_email = {\n",
    "    \"sender\": \"Crypto bro\",\n",
    "    \"subject\": \"The best investment of 2025\",\n",
    "    \"body\": \"Mr Wayne, I just launched an ALT coin and want you to buy some !\"\n",
    "}\n",
    "# Process legitimate email\n",
    "print(\"\\nProcessing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke({\n",
    "    \"email\": legitimate_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "})\n",
    "\n",
    "# Process spam email\n",
    "print(\"\\nProcessing spam email...\")\n",
    "spam_result = compiled_graph.invoke({\n",
    "    \"email\": spam_email,\n",
    "    \"is_spam\": None,\n",
    "    \"draft_response\": None,\n",
    "    \"messages\": []\n",
    "}) \n",
    "\n",
    "# Invokation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the agent via langfuse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Process legitimate email\n",
    "legitimate_result = compiled_graph.invoke(\n",
    "    input={\"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None, \"messages\": []},\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary, we built a langgraph agent workflow where we follow the following steps:\n",
    "1. Takes an incoming email\n",
    "2. Classify\n",
    "3. Handles spam\n",
    "4. Draft response for legitimate emails\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Analysis Agent in Langgraph\n",
    "\n",
    "Next, we are going to build a document analysis agent.\n",
    "\n",
    "ReAct arch - \n",
    "- act - let the model call specific tools\n",
    "- observe - pass the tool output back to the model\n",
    "- reason - let the model reason about tools to use/next steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: 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\n",
    "# delete later\n",
    "import base64\n",
    "from typing import List\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def extract_text(img_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from an image file using a multimodal model.\n",
    "\n",
    "    Args:\n",
    "        img_path: A local image file path (strings).\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the concatenated text extracted from each image.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    try:\n",
    "       \n",
    "        # Read image and encode as base64\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            image_bytes = image_file.read()\n",
    "\n",
    "        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "        # Prepare the prompt including the base64 image data\n",
    "        message = [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"Extract all the text from this image. \"\n",
    "                            \"Return only the extracted text, no explanations.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Call the vision-capable model\n",
    "        response = vision_llm.invoke(message)\n",
    "\n",
    "        # Append extracted text\n",
    "        all_text += response.content + \"\\n\\n\"\n",
    "\n",
    "        return all_text.strip()\n",
    "    except Exception as e:\n",
    "        # You can choose whether to raise or just return an empty string / error message\n",
    "        error_msg = f\"Error extracting text: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return \"\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "tools = [\n",
    "    divide,\n",
    "    extract_text\n",
    "]\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "# Use vision LLM to extract test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Any, Optional\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "class AgentState(TypedDict):\n",
    "    # The input document\n",
    "    input_file:  Optional[str]  # Contains file path, type (PNG)\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Define the langgraph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "# AgentState\n",
    "def assistant(state: AgentState):\n",
    "    # System message\n",
    "    textual_description_of_tool=\"\"\"\n",
    "extract_text(img_path: str) -> str:\n",
    "    Extract text from an image file using a multimodal model.\n",
    "\n",
    "    Args:\n",
    "        img_path: A local image file path (strings).\n",
    "\n",
    "    Returns:\n",
    "        A single string containing the concatenated text extracted from each image.\n",
    "divide(a: int, b: int) -> float:\n",
    "    Divide a and b\n",
    "\"\"\"\n",
    "    image=state[\"input_file\"]\n",
    "    sys_msg = SystemMessage(content=f\"You are an helpful agent that can analyse some images and run some computatio without provided tools :\\n{textual_description_of_tool} \\n You have access to some otpional images. Currently the loaded images is : {image}\")\n",
    "\n",
    "\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])],\"input_file\":state[\"input_file\"]}\n",
    "\n",
    "# Define the assistant node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we defined\n",
    "- tools node with list of tools\n",
    "- assistant node with assistant prompt and tools it can choose\n",
    "\n",
    "We must now create a graph with assistant and tools as the nodes.\n",
    "Use tools_condition edge to either end or tools based on assistant. Connect the tools node back to the assistant to get the loop from reAct arch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
    "\n",
    "messages = react_graph.invoke({\"messages\": messages,\"input_file\":None})\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 3: Use case for Agentic RAG\n",
    "\n",
    "### Introduction to Use Case for Agentic RAG\n",
    "\n",
    "The following represents the additions we make to the base code in the Unit_3_Agentic_RAG folder.\n",
    "\n",
    "First build a guestbook tool with \n",
    "- Load and prepare dataset\n",
    "- Create retreiver tool\n",
    "- Integrate tool with Alfred\n",
    "\n",
    "Step 1: Load and prepare the data - I will show code for all three methods - smolagents, llama-index, langgraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join([\n",
    "            f\"Name: {guest['name']}\",\n",
    "            f\"Relation: {guest['relation']}\",\n",
    "            f\"Description: {guest['description']}\",\n",
    "            f\"Email: {guest['email']}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest[\"name\"]}\n",
    "    )\n",
    "    for guest in guest_dataset\n",
    "]\n",
    "\n",
    "# Smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        text=\"\\n\".join([\n",
    "            f\"Name: {guest_dataset['name'][i]}\",\n",
    "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
    "            f\"Description: {guest_dataset['description'][i]}\",\n",
    "            f\"Email: {guest_dataset['email'][i]}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest_dataset['name'][i]}\n",
    "    )\n",
    "    for i in range(len(guest_dataset))\n",
    "]\n",
    "\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join([\n",
    "            f\"Name: {guest['name']}\",\n",
    "            f\"Relation: {guest['relation']}\",\n",
    "            f\"Description: {guest['description']}\",\n",
    "            f\"Email: {guest['email']}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest[\"name\"]}\n",
    "    )\n",
    "    for guest in guest_dataset\n",
    "]\n",
    "\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we downloaded the dataset from huggingface and then converted each guest into a document object.\n",
    "\n",
    "Step 2: Create the Retriever Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "class GuestInfoRetrieverTool(Tool):\n",
    "    name = \"guest_info_retriever\"\n",
    "    description = \"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The name or relation of the guest you want information about.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs):\n",
    "        self.is_initialized = False\n",
    "        self.retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "    def forward(self, query: str):\n",
    "        results = self.retriever.get_relevant_documents(query)\n",
    "        if results:\n",
    "            return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "        else:\n",
    "            return \"No matching guest information found.\"\n",
    "\n",
    "# Initialize the tool\n",
    "guest_info_tool = GuestInfoRetrieverTool(docs)\n",
    "# Smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n",
    "\n",
    "def get_guest_info_retriever(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.retrieve(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.text for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "# Initialize the tool\n",
    "guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)\n",
    "\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.tools import Tool\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.invoke(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")\n",
    "\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a retreival tool - using BM25Retriever (does not require embeddings) to return the most relevant guest information.\n",
    "\n",
    "Step 3: Integrate the Tool with Alfred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Create Alfred, our gala agent, with the guest info tool\n",
    "alfred = CodeAgent(tools=[guest_info_tool], model=model)\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response)\n",
    "\n",
    "# Smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# Create Alfred, our gala agent, with the guest info tool\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [guest_info_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = await alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response)\n",
    "\n",
    "# Llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Tell me about our guest named 'Lady Ada Lovelace'.\")]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)\n",
    "\n",
    "# Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements Considerations\n",
    "- improve the retriever to use sentence-transformers\n",
    "- implement conversation memory\n",
    "- combine with web search agent\n",
    "- integrate with multiple indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and Integrating Tools for the Agent \n",
    "\n",
    "First we are going to give the agent access to the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import DuckDuckGoSearchTool\n",
    "\n",
    "# Initialize the DuckDuckGo search tool\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "\n",
    "# Example usage\n",
    "results = search_tool(\"Who's the current President of France?\")\n",
    "print(results)\n",
    "\n",
    "# smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Initialize the DuckDuckGo search tool\n",
    "tool_spec = DuckDuckGoSearchToolSpec()\n",
    "\n",
    "search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)\n",
    "# Example usage\n",
    "response = search_tool(\"Who's the current President of France?\")\n",
    "print(response.raw_output[-1]['body'])\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"Who's the current President of France?\")\n",
    "print(results)\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom weather tool (schedule fireworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "import random\n",
    "\n",
    "class WeatherInfoTool(Tool):\n",
    "    name = \"weather_info\"\n",
    "    description = \"Fetches dummy weather information for a given location.\"\n",
    "    inputs = {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location to get weather information for.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, location: str):\n",
    "        # Dummy weather data\n",
    "        weather_conditions = [\n",
    "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "        ]\n",
    "        # Randomly select a weather condition\n",
    "        data = random.choice(weather_conditions)\n",
    "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = WeatherInfoTool()\n",
    "# smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "import random\n",
    "\n",
    "class WeatherInfoTool(Tool):\n",
    "    name = \"weather_info\"\n",
    "    description = \"Fetches dummy weather information for a given location.\"\n",
    "    inputs = {\n",
    "        \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The location to get weather information for.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, location: str):\n",
    "        # Dummy weather data\n",
    "        weather_conditions = [\n",
    "            {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "            {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "            {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "        ]\n",
    "        # Randomly select a weather condition\n",
    "        data = random.choice(weather_conditions)\n",
    "        return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = WeatherInfoTool()\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "import random\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = Tool(\n",
    "    name=\"get_weather_info\",\n",
    "    func=get_weather_info,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Hub Stats for Influential AI Builders\n",
    "\n",
    "Basically have context of Hugging Face Hub Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "class HubStatsTool(Tool):\n",
    "    name = \"hub_stats\"\n",
    "    description = \"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    "    inputs = {\n",
    "        \"author\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The username of the model author/organization to find models from.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, author: str):\n",
    "        try:\n",
    "            # List models from the specified author, sorted by downloads\n",
    "            models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "            \n",
    "            if models:\n",
    "                model = models[0]\n",
    "                return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "            else:\n",
    "                return f\"No models found for author {author}.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = HubStatsTool()\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n",
    "# smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "class HubStatsTool(Tool):\n",
    "    name = \"hub_stats\"\n",
    "    description = \"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    "    inputs = {\n",
    "        \"author\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The username of the model author/organization to find models from.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def forward(self, author: str):\n",
    "        try:\n",
    "            # List models from the specified author, sorted by downloads\n",
    "            models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "            \n",
    "            if models:\n",
    "                model = models[0]\n",
    "                return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "            else:\n",
    "                return f\"No models found for author {author}.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = HubStatsTool()\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally Integrate all the Tools into Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = CodeAgent(\n",
    "    tools=[search_tool, weather_info_tool, hub_stats_tool], \n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = alfred.run(\"What is Facebook and what's their most popular model?\")\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response)\n",
    "# smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "# Create Alfred with all the tools\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_tool, weather_info_tool, hub_stats_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Example query Alfred might receive during the gala\n",
    "response = await alfred.run(\"What is Facebook and what's their most popular model?\")\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response)\n",
    "\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)\n",
    "\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Gala Agent\n",
    "\n",
    "Now we combine the previous guest info retrieval, web search, weather information, and Hub States tools into one agent.\n",
    "\n",
    "The previous retrievier and tools are for the retriever.py and tools.py respectively!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "# Import our custom tools from their modules\n",
    "from tools import DuckDuckGoSearchTool, WeatherInfoTool, HubStatsTool\n",
    "from retriever import load_guest_dataset\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Initialize the web search tool\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "\n",
    "# Initialize the weather tool\n",
    "weather_info_tool = WeatherInfoTool()\n",
    "\n",
    "# Initialize the Hub stats tool\n",
    "hub_stats_tool = HubStatsTool()\n",
    "\n",
    "# Load the guest dataset and initialize the guest info tool\n",
    "guest_info_tool = load_guest_dataset()\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = CodeAgent(\n",
    "    tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], \n",
    "    model=model,\n",
    "    add_base_tools=True,  # Add any additional base tools\n",
    "    planning_interval=3   # Enable planning every 3 steps\n",
    ")\n",
    "\n",
    "# Combine all into one agent\n",
    "# smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "from tools import search_tool, weather_info_tool, hub_stats_tool\n",
    "from retriever import guest_info_tool\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "from tools import DuckDuckGoSearchRun, weather_info_tool, hub_stats_tool\n",
    "from retriever import guest_info_tool\n",
    "\n",
    "# Initialize the web search tool\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "# langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Features: Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Alfred with conversation memory\n",
    "alfred_with_memory = CodeAgent(\n",
    "    tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], \n",
    "    model=model,\n",
    "    add_base_tools=True,\n",
    "    planning_interval=3\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "response1 = alfred_with_memory.run(\"Tell me about Lady Ada Lovelace.\")\n",
    "print(\"🎩 Alfred's First Response:\")\n",
    "print(response1)\n",
    "\n",
    "# Second interaction (referencing the first)\n",
    "response2 = alfred_with_memory.run(\"What projects is she currently working on?\", reset=False)\n",
    "print(\"🎩 Alfred's Second Response:\")\n",
    "print(response2)\n",
    "\n",
    "# smolagents \n",
    "# the reset = False preserves memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Remembering state\n",
    "ctx = Context(alfred)\n",
    "\n",
    "# First interaction\n",
    "response1 = await alfred.run(\"Tell me about Lady Ada Lovelace.\", ctx=ctx)\n",
    "print(\"🎩 Alfred's First Response:\")\n",
    "print(response1)\n",
    "\n",
    "# Second interaction (referencing the first)\n",
    "response2 = await alfred.run(\"What projects is she currently working on?\", ctx=ctx)\n",
    "print(\"🎩 Alfred's Second Response:\")\n",
    "print(response2)\n",
    "\n",
    "# llama-index\n",
    "# requires adding explicity context object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction\n",
    "response = alfred.invoke({\"messages\": [HumanMessage(content=\"Tell me about 'Lady Ada Lovelace'. What's her background and how is she related to me?\")]})\n",
    "\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)\n",
    "print()\n",
    "\n",
    "# Second interaction (referencing the first)\n",
    "response = alfred.invoke({\"messages\": response[\"messages\"] + [HumanMessage(content=\"What projects is she currently working on?\")]})\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)\n",
    "\n",
    "# langgraph\n",
    "# use the memorySaver component; in this example it was just concatenation of the previous messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Notes\n",
    "\n",
    "Build an agent and evaluate on the GAIA Benchmark.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
